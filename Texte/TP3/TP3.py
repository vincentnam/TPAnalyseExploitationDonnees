import numpy
import os
import codecs
import re
import nltk
from nltk.corpus import stopwords
import pandas
import operator
import matplotlib.pyplot as plt


# Fonction pour les statistiques de bases du fichier texte
def basic_stat(texte):
    '''
    Affiche les statistiques de base sur le texte en entrée :
    Le nombre de ligne, le dictionnaire des apparitions de mots trié,
    le nombre de mots dans le texte
    :param texte: liste de liste de mots
    :return: None
    '''
    stat={}
    print("nombre de ligne = " + str(len(texte)))
    for phrase in texte:
        for mot in remove_stopword(phrase):
            if mot not in stat:
                stat[mot]= 1
            else:
                stat[mot]+=1
    aux = 0
    sorted_stat = sorted(stat.items(), key=operator.itemgetter(1), reverse=True)
    print(sorted_stat)
    for mot in sorted_stat:
        aux+= mot[1]
    print()
    print("Il y a " + str(aux) + " mots dans le texte.")

    return None

def get_glove():
    dic = {}
    for x in open("glove.6B.50d.txt").readlines():
        x = x.split()
        dic[x[0]] = numpy.array([float(w) for w in x[1:]])
    return dic

def remove_stopword(phrase):
    aux_list = numpy.array([])
    stoplist = stopwords.words('english')
    #print(phrase)
    #print(stoplist)
    for word in phrase :
        if word.lower() not in stoplist:
            
            aux_list = numpy.append(aux_list, [word])
#            print(aux_list)

#    print(aux_list)
    return aux_list
#On suppose qu'on a un texte qui est une liste de phrases
# Ces phrases sont des phrases sans stopword, sans ponctuation
# et considérées comme une liste de mot (liste de liste)
def tfidf_dict(texte):
    dict_tf={}
    dict_idf={}
    compt = 0
    for phrase in texte :
        vu = []
        compt += len(phrase)
        for mot in phrase :

            if mot in dict_tf:
                dict_tf[mot.lower()]+=1
                if mot not in vu :
                    vu.append(mot.lower())
            else:
                dict_tf[mot.lower]=1
                if mot not in vu :
                    vu.append(mot.lower())
        for apparition in vu:
            if apparition in dict_idf:
                dict_idf[apparition.lower()] += 1
            else:
                dict_idf[apparition.lower()] = 1
    dict_res = {}
    for mot in dict_idf:
        dict_res[mot.lower()] = (dict_tf[mot.lower()]/compt )*( dict_idf[mot.lower()]/len(texte))
    return dict_res
'''
def tfidf(mot, texte):
    compt_mot = 0
    compt_tot = 0
    nbi=0
    compt_phrase = 0

    for sentence in texte :
        present = 0
        for word in sentence:
            if word == mot:
                present = 1
                compt_mot+=1
            compt_tot+=1
        nbi += present
        compt_phrase += 1
    tf = compt_mot/compt_tot
    if nbi != 0 :
        idf = compt_phrase / nbi
    else:
        return -10000
    return tf*idf
'''
''' 
def poids_phrase(phrase, corpus):
    # C'est ici qu'on peut améliorer
    compt = 0
    for mot in phrase:
        compt = tfidf(mot,corpus)
    return compt / len(phrase)
'''

def resume(k, corpus, sim="tfidf"):
    list_resume = []
    if sim=="tfidf":
        dict_sim = tfidf_dict(corpus)
    elif sim=="glove":
        dict_sim = get_glove()
    else:
        dict_sim = tfidf_dict(corpus)
    dict_pen = {}

    list_indic = numpy.array([])
    for boucle in range(k):
        vec_poids = numpy.array([])
        for phrase in corpus:
            poids_phrase = 0

            # On utilise le poids moyen des tfidf de la phrase
            for mot in remove_stopword(phrase):
                # On s'occupe de lire la pénalité si il y a
                if mot.lower() in dict_sim:
                    if mot in dict_pen:
                        # ATTENTION ! Glove : get sim pour un mot avec tous les autres pour tous les mots de la phrase

                        poids_phrase += dict_sim[mot.lower()] * (0.01**dict_pen[mot.lower()])
                    else:
                        poids_phrase += dict_sim[mot.lower()]
            vec_poids = numpy.append(vec_poids, poids_phrase / len(phrase))

            '''
            # On utilise le max des tfidf de la phrase
            for mot in remove_stopword(phrase) :
                if poids_phrase < dict_tfidf[mot]:
                    poids_phrase = dict_tfidf[mot]
            vec_poids = numpy.append(vec_poids, poids_phrase/len(phrase))

            # On utilise le max des tfidf de la phrase
            for mot in remove_stopword(phrase) :
                if poids_phrase > dict_tfidf[mot]:
                    poids_phrase = dict_tfidf[mot]
            vec_poids = numpy.append(vec_poids, poids_phrase/len(phrase))

            '''

        aux = vec_poids[0]
        for j in range(len(vec_poids)):
            if vec_poids[j] > aux and j not in list_indic:
                aux = vec_poids[j]
                indice = j
        list_indic = numpy.append(list_indic, indice)
        list_resume.append(corpus[indice])

        for mot in remove_stopword(corpus[indice]):
            if mot not in dict_pen:
                dict_pen[mot.lower()] = 1
            else :
                dict_pen[mot.lower()] +=1

        numpy.delete(corpus, indice)
    return list_resume





def lect_text(path):
    text = numpy.array([])
    for x in open(path,"r").readlines():
        text = numpy.append(text, x)
    return text


list_files=numpy.array([])

list_sys=numpy.array([])

for _,_,file in os.walk("./Critiques/projects/test-summarization/topics/"):
    list_files = numpy.append(list_files,file)
for _,_,file in os.walk("./Critiques/projects/test-summarization/system/"):
    list_sys = numpy.append(list_sys, file)
print(list_files)
print(list_sys)
for i in range(len(list_files)) :
    list_phrase = []
    for x in open("./Critiques/projects/test-summarization/topics/" + list_files[i],"r+", encoding="ISO-8859-1").readlines():
        # list_phrase = numpy.append(list_phrase, numpy.array(re.sub(r"[\n,.]+", ' ', x).split()))
        list_phrase.append(numpy.array(re.sub(r"[\n,!,?,.]+", ' ', x).split()))
    list_phrase = numpy.array(list_phrase)
    #  print(tfidf_dict(list_phrase))
    coucou = (resume(5,list_phrase))
    print(list_files[i])
    for coucoux in coucou :
        print(coucoux)
    print("")

    basic_stat(list_phrase)
    #print(list_phrase)
    resumevar = resume(5, list_phrase, sim="glove")
    for j in range(len(resumevar)):
        resumevar[j]=" ".join(resumevar[j].tolist())

    print("ON ECRIT DANS : ./Critiques/projects/test-summarization/system/" +list_sys[i])
    f=open("./Critiques/projects/test-summarization/system/" +list_sys[i],"w")
    for item in resumevar:
        f.write("%s\n" % item)
    f.close()
    #print(list_phrase)
    #for phrase in list_phrase:

    # print(list_phrase)
    #print(phrase)
    #remove_stopword(phrase.split())

'''
list_aux = []
for x in open("./Critiques/projects/test-summarization/topics/speed_windows7.txt.data", "r+", encoding="ISO-8859-1").readlines():
    list_aux.append(re.sub(r"[\n,.]+", ' ', x).split())


print(resume(5, list_aux))

'''