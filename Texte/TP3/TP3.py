import numpy
import os
import codecs
import re
import nltk
from nltk.corpus import stopwords
import pandas
import operator
import matplotlib.pyplot as plt



# Fonction de similiarité pour les words embbeding : glove
def ESA(u,v):
    return numpy.dot(u,v)/(numpy.linalg.norm(u) * numpy.linalg.norm(v))


def glove_sim_all_texte(mot,voc,glove):
    '''
    On renvoie la moyenne des valeurs de glove pour la totalité du texte
    pondéré par la fréquence des mots comparés (un mot revenant souvant
    aura plus de poids sur la similarité entre le mot et le texte)
    :param mot: mot à comparer au texte
    :param voc: dictionnaire contenant le vocabulaire en clé et le nombre
    d'occurrences en valeur
    :param glove: liste des glove vecteurs
    :return:
    '''
    sim = 0
    # On peut se demander si on veut la fréquence d'un mot dans le vocabulaire
    # (i.e. le texte sans les stop word) ou dans la totalité du texte
    # (i.e. le texte avec les stop word); ici on décide de ne pas considérer
    # les stop word
    occur_tot = 0
    # On effectue le comptage du nombre de mots avant pour éviter tout
    # problème sur l'encodage des entier en python (qui ne devrait pas
    # arriver normalement
    for mot in voc :
        occur_tot += voc[mot]
    for mot_voc in voc :
        if mot_voc in glove :
            sim += ESA(glove[mot.lower()], glove[mot_voc.lower()]) * voc[mot_voc] / occur_tot
    return sim

# Renvoie le vocabulaire du texte
def get_voc(texte):
    """
    Renvoie le vocabulaire du texte ainsi que les nombre d'occurrences
    de chaque mot
        :param texte: texte: list de mots sans les charactères spéciaux
        ("/n", ".", "?"...)
        :return:dict_voc : un dictionnaire contenant la liste des mots uniques
        du texte ainsi que leur indice dans la matrice de PPMI
    """
    occur_mot = {}
    for phrase in texte:
        for mot in remove_stopword(phrase):
            if mot not in occur_mot:
                occur_mot[mot] = 1
            else:
                occur_mot[mot] += 1
    return occur_mot

# Fonction pour les statistiques de bases du fichier texte
def basic_stat(texte):
    '''
    Affiche les statistiques de base sur le texte en entrée :
    Le nombre de ligne, le dictionnaire des apparitions de mots trié,
    le nombre de mots dans le texte
    :param texte: liste de liste de mots
    :return: None
    '''
    stat={}
    print("nombre de ligne = " + str(len(texte)))
    for phrase in texte:
        for mot in remove_stopword(phrase):
            if mot not in stat:
                stat[mot]= 1
            else:
                stat[mot]+=1
    aux = 0
    sorted_stat = sorted(stat.items(), key=operator.itemgetter(1), reverse=True)
    print(sorted_stat)
    for mot in sorted_stat:
        aux += mot[1]

    voc = get_voc(texte)
    print("Il y a " + str(aux) + " mots dans le texte.")
    print("Il y a " + str(len(voc))+ " mots différents dans ce texte. ")
    print("Le mot apparaissant le plus souvent ("+ str(sorted_stat[0][1])+" fois) est \""+sorted_stat[0][0]+"\".")

    return None

def get_glove():
    dic = {}
    for x in open("glove.6B.50d.txt").readlines():
        x = x.split()
        dic[x[0]] = numpy.array([float(w) for w in x[1:]])
    return dic

def remove_stopword(phrase):
    aux_list = numpy.array([])
    stoplist = stopwords.words('english')
    #print(phrase)
    #print(stoplist)
    for word in phrase :
        if word.lower() not in stoplist:
            
            aux_list = numpy.append(aux_list, [word])
#            print(aux_list)

#    print(aux_list)
    return aux_list


#On suppose qu'on a un texte qui est une liste de phrases
# Ces phrases sont des phrases sans stopword, sans ponctuation
# et considérées comme une liste de mot (liste de liste)
def tfidf_dict(texte):
    dict_tf={}
    dict_idf={}
    compt = 0
    for phrase in texte :
        vu = []
        compt += len(phrase)
        for mot in phrase :

            if mot in dict_tf:
                dict_tf[mot.lower()]+=1
                if mot not in vu :
                    vu.append(mot.lower())
            else:
                dict_tf[mot.lower]=1
                if mot not in vu :
                    vu.append(mot.lower())
        for apparition in vu:
            if apparition in dict_idf:
                dict_idf[apparition.lower()] += 1
            else:
                dict_idf[apparition.lower()] = 1
    dict_res = {}
    for mot in dict_idf:
        dict_res[mot.lower()] = (dict_tf[mot.lower()]/compt )*( dict_idf[mot.lower()]/len(texte))
    return dict_res


def resume(k, corpus, sim="tfidf"):
    list_resume = []
    if sim=="tfidf":
        dict_sim = tfidf_dict(corpus)
        dict_pen = {}

        list_indic = numpy.array([])
        for boucle in range(k):
            vec_poids = numpy.array([])
            for phrase in corpus:
                poids_phrase = 0

                # On utilise le poids moyen des tfidf de la phrase
                for mot in remove_stopword(phrase):
                    # On s'occupe de lire la pénalité si il y a
                    if mot.lower() in dict_sim:
                        if mot.lower() in dict_pen:
                            # ATTENTION ! Glove : get sim pour un mot avec tous les autres pour tous les mots de la phrase

                            poids_phrase += dict_sim[mot.lower()] * (
                                        0.01 ** dict_pen[mot.lower()])
                        else:
                            poids_phrase += dict_sim[mot.lower()]
                vec_poids = numpy.append(vec_poids, poids_phrase / len(phrase))

                '''
                # On utilise le max des tfidf de la phrase
                for mot in remove_stopword(phrase) :
                    if poids_phrase < dict_tfidf[mot]:
                        poids_phrase = dict_tfidf[mot]
                vec_poids = numpy.append(vec_poids, poids_phrase/len(phrase))

                # On utilise le max des tfidf de la phrase
                for mot in remove_stopword(phrase) :
                    if poids_phrase > dict_tfidf[mot]:
                        poids_phrase = dict_tfidf[mot]
                vec_poids = numpy.append(vec_poids, poids_phrase/len(phrase))

                '''

            aux = vec_poids[0]
            for j in range(len(vec_poids)):
                if vec_poids[j] > aux and j not in list_indic:
                    aux = vec_poids[j]
                    indice = j
            list_indic = numpy.append(list_indic, indice)
            list_resume.append(corpus[indice])

            for mot in remove_stopword(corpus[indice]):
                if mot not in dict_pen:
                    dict_pen[mot.lower()] = 1
                else:
                    dict_pen[mot.lower()] += 1

            numpy.delete(corpus, indice)
        return list_resume

    elif sim=="glove":
        dict_sim = get_glove()
        # On récupère le vocabulaire du texte sans les stop_words
        # Pour récupérer la valeur de ESA d'un mot pour tous les mots du
        # corpus : on souhaite voir la similarité du mot avec chacun la
        # totalité du corpus
        voc = get_voc(corpus)
        dict_pen = {}
        dict_glove_mot = {}
        for phrase in corpus:
            for mot in phrase :
                if mot not in dict_glove_mot:
                    dict_glove_mot[mot.lower()] = glove_sim_all_texte(mot,voc,dict_sim)
        list_indic = numpy.array([])
        for boucle in range(k):
            vec_poids = numpy.array([])
            for phrase in corpus:
                poids_phrase = 0

                # On utilise le poids moyen des tfidf de la phrase
                for mot in remove_stopword(phrase):
                    # On s'occupe de lire la pénalité si il y a
                    if mot.lower() in dict_sim:
                        if mot in dict_pen:

                            poids_phrase += dict_glove_mot[mot.lower()]*(1 ** dict_pen[mot.lower()])
                        else:
                            poids_phrase += dict_glove_mot[mot.lower()]
                # On garde la valeur moyenne de similarité des mots par rapport
                # au texte
                vec_poids = numpy.append(vec_poids, poids_phrase / len(phrase))

            aux = vec_poids[0]
            for j in range(len(vec_poids)):
                if vec_poids[j] > aux and j not in list_indic:
                    aux = vec_poids[j]
                    indice = j
            list_indic = numpy.append(list_indic, indice)
            list_resume.append(corpus[indice])

            for mot in remove_stopword(corpus[indice]):
                if mot not in dict_pen:
                    dict_pen[mot.lower()] = 1
                else:
                    dict_pen[mot.lower()] += 1

            numpy.delete(corpus, indice)
        return list_resume

    else:
        raise Exception("Méthode de similarité non reconnue.")





def lect_text(path):
    text = numpy.array([])
    for x in open(path,"r").readlines():
        text = numpy.append(text, x)
    return text


list_files=numpy.array([])

list_sys=numpy.array([])

for _,_,file in os.walk("./Critiques/projects/test-summarization/topics/"):
    list_files = numpy.append(list_files,file)
for _,_,file in os.walk("./Critiques/projects/test-summarization/system/"):
    list_sys = numpy.append(list_sys, file)
print(list_files)
print(list_sys)
for i in range(len(list_files)) :
    list_phrase = []
    for x in open("./Critiques/projects/test-summarization/topics/" + list_files[i],"r+", encoding="ISO-8859-1").readlines():
        # list_phrase = numpy.append(list_phrase, numpy.array(re.sub(r"[\n,.]+", ' ', x).split()))
        list_phrase.append(numpy.array(re.sub(r"[\n,!,?,.,|]+", ' ', x).split()))
    list_phrase = numpy.array(list_phrase)
    #  print(tfidf_dict(list_phrase))


    basic_stat(list_phrase)
    #print(list_phrase)
    resumevar = resume(5, list_phrase, sim="glove")
    for j in range(len(resumevar)):
        resumevar[j]=" ".join(resumevar[j].tolist())
    print(resumevar)
    print("ON ECRIT DANS : ./Critiques/projects/test-summarization/system/" +list_sys[i])
    f=open("./Critiques/projects/test-summarization/system/" +list_sys[i],"w")
    for item in resumevar:
        f.write("%s\n" % item)
    f.close()
    #print(list_phrase)
    #for phrase in list_phrase:

    # print(list_phrase)
    #print(phrase)
    #remove_stopword(phrase.split())

'''
list_aux = []
for x in open("./Critiques/projects/test-summarization/topics/speed_windows7.txt.data", "r+", encoding="ISO-8859-1").readlines():
    list_aux.append(re.sub(r"[\n,.]+", ' ', x).split())


print(resume(5, list_aux))

'''